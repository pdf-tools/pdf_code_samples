# ðŸ” PDF Search Engine with Pdftools Conversion Service and Elasticsearch

This repository provides tools and instructions for creating a simple [Elasticsearch](https://www.elastic.co/)-based search engine from the output of the [Pdftools Conversion Service](https://www.pdf-tools.com/products/conversion/pdf-tools-conversion-service/). It illustrates the essentials of how to build a search engine for PDFs generated by the Pdftools Conversion Service.

By combining Elasticsearch with Pdftools Conversion Service, you can easily search through documents you've converted, filter by metadata, and visualize data using Kibana. This project includes a PDFIngestor in C# to handle PDF ingestion and a React-based frontend powered by [Searchkit](https://www.searchkit.co/).

Given the the Conversion Service is capable of converting a wide range of document formats, includiing Word, Excel, PowerPoint, images with OCR, and more, PDF is a good standard output format to build a uniform search experience.

## Key Features
- **Full-text Search for PDFs** â€“ Leverage Elasticsearch to make PDF content searchable.
- **Metadata Extraction** â€“ Extract key metadata like authors, conformance levels, and fonts from PDFs.
- **Faceted Search** â€“ Filter results based on metadata fields.
- **Modern Frontend** â€“ A sleek, React-based UI using Searchkit for easy searching and filtering.
- **Simple Setup** â€“ Dockerized Elasticsearch, Kibana, and Logstash for quick local deployment.

## Architecture Overview

This is a rough overview of how the system works:

```
(Incoming Documents) -->
    [Pdftools Conversion Service] -->
        (Converted PDFs) -->
            [PDFIngestor] -->
                (PDF to Text and Metadata) -->
                    [Elasticsearch] <-- [React Frontend]
```

Note that we don't store the original PDFs in Elasticsearch here. We use the [Pdftools SDK](https://www.pdf-tools.com/products/conversion/pdf-tools-sdk/) to extract metadata and text from the converted PDFs.

## Getting Started

### Prerequisites
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) (recommended for local development)
- [Node.js](https://nodejs.org/) and [npm](https://www.npmjs.com/) (for frontend development)
- [.NET 8 SDK](https://dotnet.microsoft.com/en-us/download/dotnet/8.0) (for building the PDFIngestor)
- [Pdftools SDK](https://www.pdf-tools.com/docs/pdf-tools-sdk/getting-started/pdftools-sdk/) and the [Toolbox Add-On](https://www.pdf-tools.com/docs/pdf-tools-sdk/getting-started/toolbox/) (for processing PDFs)

### Installing Elasticsearch

First we need to set up a local Elasticsearch instance. The easiest way is to use [docker-elk](https://github.com/deviantony/docker-elk), which is designed to help you get up and running quickly.

1. Clone the [docker-elk](https://github.com/deviantony/docker-elk) repository:

   ```bash
   git clone https://github.com/deviantony/docker-elk
   cd docker-elk
   ```

2. Run the setup and start Elasticsearch:

   ```bash
   docker compose up setup
   docker compose up -d
   ```

Note that the `-d` option is used to run Elasticsearch in the background. You can check the Elasticsearch logs by looking in Docker Desktop at the `docker-elk` container or by running `docker logs -f docker-elk-elasticsearch-1`. 

3. Elasticsearch will be available at [http://localhost:9200](http://localhost:9200) and Kibana at [http://localhost:5601](http://localhost:5601) (for this tutorial we won't be using Kibana)

4. Edit the Elasticsearch configuration to enable CORS (for frontend access):

    ```yaml
    # docker-elk/elasticsearch/config/elasticsearch.yml
    ---
    ## Default Elasticsearch configuration from Elasticsearch base image.
    ## https://github.com/elastic/elasticsearch/blob/main/distribution/docker/src/docker/config/elasticsearch.yml
    #
    cluster.name: docker-cluster
    network.host: 0.0.0.0

    ## X-Pack settings
    ## see https://www.elastic.co/guide/en/elasticsearch/reference/current/security-settings.html
    #
    xpack.license.self_generated.type: trial
    ## disable security for development (not recommended for production)
    xpack.security.enabled: false

    # Add http cors
    http.cors:
    enabled: true
    allow-origin: /https?:\/\/localhost(:[0-9]+)?/
    ```

Note that setting `xpack.security.enabled: false` is not secure! We do this only for develepment to get up and running quickly.

5. Restart the Elasticsearch container to apply changes.

    ```bash
    docker compose down
    docker compose up -d
    ```

### Configuring Your Elasticsearch Index and Server

1. Create an index called `convsrv` in Elasticsearch:

    ```bash
    curl -X PUT "http://localhost:9200/convsrv" -H "Content-Type: application/json" -d '
    {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 1
        },
        "mappings": {
            "properties": {
                "author": {
                    "type": "keyword"
                },
                "numberOfPages": {
                    "type": "integer"
                },
                "conformance": {
                    "type": "keyword"
                },
                "fontNames": {
                    "type": "keyword"
                }
            }
        }
    }'
    ```

Note your endpoint is `http://localhost:9200/convsrv`

Note if you want to delete the index again and start fresh: `curl -X DELETE "http://localhost:9200/convsrv"`

**Breakdown:**

- `PUT`: Creates the index.
- `http://localhost:9200/convsrv`: The endpoint for creating an index named `convsrv`.
- `number_of_shards`: Number of primary shards for the index (1 is typical for development).
- `number_of_replicas`: Number of replica copies (1 for redundancy, set to 0 if you want to save resources).
- `mappings`: Defines the properties and types for each field (author, numberOfPages, conformance, fontNames). These enable facetted search and sorting.

2. Verify that the index was created correctly:

    ```bash
    curl -X GET "http://localhost:9200/_cat/indices?v"
    ```
... you should see `convsrv` in the list

    ```bash
    curl -X GET "http://localhost:9200/convsrv/_mapping?pretty"
    ```

... and the mappings you set up for faceted search

3. Configure the Elasticsearch Server

    ```bash
    curl -X PUT -H "Content-Type: application/json" http://localhost:9200/_cluster/settings -d '
        {
        "transient": {
            "cluster.routing.allocation.disk.threshold_enabled": false
        }
    }'
    ```

    ```bash
    curl -X PUT -H "Content-Type: application/json" http://localhost:9200/_all/_settings -d '
    {
    "index.blocks.read_only_allow_delete": null
    }'
    ```

**Why are these needed?**
- `disk.threshold_enabled: false`: Prevents Elasticsearch from blocking shard allocation even if disk space is low. Useful in development but not recommended in production.
- `read_only_allow_delete: null`: Lifts the read-only block that Elasticsearch automatically applies when the disk is too full. This allows indices to be written to again.

### Build and Run PDFIngestor

The PDFIngestor is a .NET application that processes PDFs and indexes them into Elasticsearch. It requires:

##### Prerequisites

- [.NET 8 SDK](https://dotnet.microsoft.com/en-us/download/dotnet/8.0) or later
- [Pdftools SDK](https://www.pdf-tools.com/products/conversion/pdf-tools-sdk/)
- [Pdftools Toolbox Add-On](https://www.pdf-tools.com/docs/pdf-tools-sdk/getting-started/toolbox/)
- A valid Pdftools license key (obtain an evaluation license from [pdf-tools.com](https://www.pdf-tools.com))

**Note:** PDFIngestor currently only supports HTTP Basic authentication for Elasticsearch.

##### Setup

1. Install the Pdftools SDK and Toolbox Add-On following the instructions on their respective documentation pages.
2. Navigate to the PDFIngestor directory:
   ```bash
   cd PDFIngestor/PDFIngestor
   ```
3. Copy `appsettings.example.json` to `appsettings.json`:
   ```bash
   cp appsettings.example.json appsettings.json
   ```
4. Edit `appsettings.json` and add your Pdftools license key:
   ```json
   {
     "PdfTools": {
       "LicenseKey": "your-license-key-here"
     }
   }
   ```
5. Build the project:
   ```bash
   dotnet build
   ```
6. Run the PDFIngestor:
   ```bash
   cd bin/Debug/net8.0
   ./PDFIngestor watch ~/pdf_files
   ```
##### Running
The PDFIngestor can operate in two modes:

1. For Watch Mode:
   - Place PDF files in the watched directory
   - The PDFIngestor will automatically process all files already in the folder and any new ones that are added
   - Use Ctrl+C to stop watching

2. For Execute Mode:
   - Specify one or more PDF files as arguments
   - The PDFIngestor will process all specified files then exit
   - Ideal for automation and integration with the Conversion Service

Get help with the PDFIngestor:

    ```bash
    ./PDFIngestor -h # Docker
    PDFIngestor.exe -h # Windows
    ```

    ```
    PDFIngestor - A tool to ingest PDF files into Elasticsearch

    Usage:
    PDFIngestor watch <folder>      Watch a folder for new PDF files
    PDFIngestor exec <file1> [file2...]  Process one or more PDF files

    Optional Parameters:
    -e, --endpoint <url>   Elasticsearch endpoint URL
                            (default: http://localhost:9200/convsrv/_doc)
    -u, --username <user>  Elasticsearch username
    -p, --password <pass>  Elasticsearch password

    Environment Variables:
    The following environment variables can be set in a .env file:
    ELASTIC_ENDPOINT  - Elasticsearch endpoint URL
    ELASTIC_USER     - Elasticsearch username
    ELASTIC_PASSWORD - Elasticsearch password
    ```

###### Watch Mode
    ```bash
    ./PDFIngestor watch <input-directory> -u <elasticsearch-username> -p <elasticsearch-password> -e <elasticsearch-endpoint>
    ```

**Note:** the example Elasticsearch configuration we used has authentication disabled, so you can omit the `-u` and `-p` flags.

Example:
    ```bash
    ./PDFIngestor watch ./sample-pdfs -e https://localhost:9200/convsrv/_doc
    ```

**Note:** the endpoint URL here is the _default_ endpoint PDFIngestor will use if no `-e` flag is specified.

###### Execute Mode
    ```bash
    ./PDFIngestor exec <file1> [file2...]
    ```

Example:
    ```bash
    ./PDFIngestor exec ./document1.pdf ./document2.pdf
    ```

###### Integration with Pdftools Conversion Service
The PDFIngestor can be integrated with the Pdftools Conversion Service using the [Execute Command connector](https://www.pdf-tools.com/docs/conversion-service/integrate/connectors/other-connectors/). Configure it as follows:

1. In the Conversion Service configuration, add an "Execute Command" output connector
2. Set the command to point to your PDFIngestor executable
3. Use the following argument pattern e.g.

    ```bash
    /path/to/PDFIngestor exec [output:FILES]
    ```

    ```
    Command: C:\Path\To\PDFIngestor.exe
    Arguments: exec [output:FILES]
    ```

The `[output:FILES]` placeholder will be replaced by the Conversion Service with a space-separated list of converted PDF files.

You can also use `[output:FILE_1]` to process only the first converted file, or `[output:FILE_2]` for the second, etc.


### Run the Frontend

1. Navigate to the frontend directory and start the development server:
   ```bash
   cd with-ui-nextjs-react
   npm run dev
   ```
2. Access the frontend at [http://localhost:3000](http://localhost:3000).

#### Frontend Configuration

The frontend is built using Next.js and Searchkit. The main configuration files are:

- `app/api/search/route.ts`: Configures the Elasticsearch connection, search settings, and facet attributes
- `app/page.tsx`: Defines the search UI layout and components, including refinement lists and search results
- `app/layout.tsx`: Contains the base layout and global styles

For detailed information about customizing the search UI, including adding new facets, modifying the search behavior, or styling components, refer to the [Searchkit documentation](https://www.searchkit.co/).

## Future Improvements

This project serves as a prototype to demonstrate the integration between PDF-Tools Conversion Service and Elasticsearch. While it provides basic search functionality, there are several potential improvements that could enhance its capabilities:

### Document Viewing
The most significant improvement would be integrating the [Pdftools Web Viewer](https://www.pdf-tools.com/products/viewing-printing/pdf-web-viewer/) into the search results. This would provide a professional-grade viewing experience directly in the browser, allowing users to:
- View PDF documents with high fidelity
- Search within documents
- Navigate through multi-page documents
- And much more...

For production use cases, this integration would significantly enhance the user experience by providing a seamless transition from search results to document viewing.

## Feedback
This project is a proof of concept and is not intended for production use. Contributions, feedback, and ideas are welcome to improve the solution further.

For questions please to reach us via [our website](https://www.pdf-tools.com/contact/).
